{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from wombat.engine.ml_model import canonical_query\n",
    "from wombat.models import engine\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "res = engine.execute(canonical_query).fetchall()\n",
    "\n",
    "info_blobs = []\n",
    "for r in res[0:5]:\n",
    "    info_blobs.append([r[2], r[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lurex dress tibi detailed with metallic gold fil coupe, this flowing maxi dress bares the shoulders with a halter neck and falls loosely into a pleated ruffle hem.\n"
     ]
    }
   ],
   "source": [
    "doc_set = []\n",
    "for i, t in enumerate(info_blobs):\n",
    "    # get first sentence from description\n",
    "    first_sentence = re.split(r'(?<=[.:;])\\s', t[1])[0]\n",
    "    item_title = info_blobs[i][0]\n",
    "    combined = ' '.join([item_title, first_sentence])\n",
    "    doc_set.append(combined.lower())\n",
    "print(doc_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.074*\"dress\" + 0.074*\"ava\" + 0.074*\"mini\" + 0.042*\"layer\" + 0.042*\"parti\" + 0.042*\"perfect\" + 0.042*\"portrait\" + 0.042*\"heel\" + 0.042*\"tight\" + 0.042*\"self\" + 0.042*\"lace\" + 0.042*\"guipur\" + 0.042*\"weather\" + 0.042*\"season\" + 0.042*\"cool\" + 0.042*\"s\" + 0.042*\"team\" + 0.011*\"coup\" + 0.011*\"bare\" + 0.011*\"loos\" + 0.011*\"maxi\" + 0.011*\"metal\" + 0.011*\"fall\" + 0.011*\"lurex\" + 0.011*\"neck\" + 0.011*\"tibi\" + 0.011*\"fil\" + 0.011*\"flow\" + 0.011*\"hem\" + 0.011*\"ruffl\" + 0.011*\"gold\" + 0.011*\"shoulder\" + 0.011*\"pleat\" + 0.011*\"detail\" + 0.011*\"halter\"'), (1, '0.074*\"dress\" + 0.042*\"halter\" + 0.042*\"detail\" + 0.042*\"pleat\" + 0.042*\"shoulder\" + 0.042*\"gold\" + 0.042*\"ruffl\" + 0.042*\"hem\" + 0.042*\"flow\" + 0.042*\"tibi\" + 0.042*\"fil\" + 0.042*\"neck\" + 0.042*\"lurex\" + 0.042*\"fall\" + 0.042*\"metal\" + 0.042*\"maxi\" + 0.042*\"loos\" + 0.042*\"bare\" + 0.042*\"coup\" + 0.011*\"s\" + 0.011*\"cool\" + 0.011*\"guipur\" + 0.011*\"season\" + 0.011*\"team\" + 0.011*\"self\" + 0.011*\"weather\" + 0.011*\"lace\" + 0.011*\"tight\" + 0.011*\"perfect\" + 0.011*\"heel\" + 0.011*\"parti\" + 0.011*\"portrait\" + 0.011*\"layer\" + 0.011*\"mini\" + 0.011*\"ava\"'), (2, '0.029*\"dress\" + 0.029*\"team\" + 0.029*\"fall\" + 0.029*\"pleat\" + 0.029*\"bare\" + 0.029*\"maxi\" + 0.029*\"s\" + 0.029*\"fil\" + 0.029*\"shoulder\" + 0.029*\"flow\" + 0.029*\"cool\" + 0.029*\"season\" + 0.029*\"coup\" + 0.029*\"weather\" + 0.029*\"metal\" + 0.029*\"gold\" + 0.029*\"halter\" + 0.029*\"loos\" + 0.029*\"neck\" + 0.029*\"tight\" + 0.029*\"ruffl\" + 0.029*\"lace\" + 0.029*\"guipur\" + 0.029*\"heel\" + 0.029*\"lurex\" + 0.029*\"portrait\" + 0.029*\"tibi\" + 0.029*\"detail\" + 0.029*\"hem\" + 0.029*\"self\" + 0.029*\"perfect\" + 0.029*\"parti\" + 0.029*\"layer\" + 0.029*\"mini\" + 0.029*\"ava\"')]\n"
     ]
    }
   ],
   "source": [
    "en_stop = get_stop_words('en')\n",
    "\n",
    "texts = []\n",
    "for doc in doc_sets[0:2]:\n",
    "\n",
    "    # tokenize document string\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    p_stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "    texts.append(stemmed_tokens)\n",
    "    \n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def hellinger(X):\n",
    "    return squareform(pdist(np.sqrt(X)))/np.sqrt(2)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "X = ldamodel.state.get_lambda()\n",
    "X = X / X.sum(axis=1)[:, np.newaxis]\n",
    "h = hellinger(X)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=3, num_words=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
